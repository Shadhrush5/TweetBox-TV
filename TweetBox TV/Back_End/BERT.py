# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kZuGoz2Rv0xcFQDAeCXS8lj3m7uqP8zF
"""

!pip3 install transformers
!pip3 install faiss-cpu
!pip3 install -U scikit-learn scipy matplotlib
!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu

import torch
import pandas as pd
import faiss
import time
from transformers import BertTokenizer, BertModel

# BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Load the sentences from json file
tweets = pd.read_json("data.json")

def embedding_creator(tokens):
    tokens['input_ids'] = torch.stack(tokens['input_ids'])
    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])
    with torch.no_grad():
        outputs = model(**tokens)
    embeddings = outputs.last_hidden_state
    attention_mask = tokens['attention_mask']
    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
    masked_embeddings = embeddings * mask
    summed = torch.sum(masked_embeddings, 1)
    summed_mask = torch.clamp(mask.sum(1), min=1e-9)
    mean_pooled = summed / summed_mask
    return mean_pooled

def process_batch(tweets):
    tokens = {'input_ids': [], 'attention_mask': []}
    for sentence in tweets:
        # Encode each sentence and append to the dictionary
        new_tokens = tokenizer.encode_plus(sentence, max_length=512,
                                           truncation=True, padding='max_length',
                                           return_tensors='pt')
        tokens['input_ids'].append(new_tokens['input_ids'][0])
        tokens['attention_mask'].append(new_tokens['attention_mask'][0])
    outputs = embedding_creator(tokens)
    result = [outputs]
    return result


# Create batches of sentences to process
batch_size = 2500
batches = [tweets[i:i + batch_size] for i in range(0, len(tweets), batch_size)]

# Process each batch
result = []
start_time = time.time()
for i, batch in enumerate(batches):
    print(f"Processing batch {i+1}/{len(batches)}")
    batch_result = process_batch(batch)
    result.extend(batch_result)

print("Time taken: ", time.time() -  start_time)

# FAISS indexing
d = 768
xb = torch.cat(result).numpy()
index = faiss.IndexFlatIP(d)
index.add(xb)
print("Total number of indexed sentences: ", index.ntotal)

# Function to convert a query to an embedding
def convert_to_embedding(query):
    tokens = {'input_ids': [], 'attention_mask': []}
    new_tokens = tokenizer.encode_plus(query, max_length=512,
                                       truncation=True, padding='max_length',
                                       return_tensors='pt')
    tokens['input_ids'].append(new_tokens['input_ids'][0])
    tokens['attention_mask'].append(new_tokens['attention_mask'][0])
    return embedding_creator(tokens)

# Search funtion and gets query from user
def search_similar_sentences(query, k=5):
    query_embeddings = convert_to_embedding(query).numpy()
    distances, indices = index.search(query_embeddings, k)
    for i in range(k):
        print("Similarity Score:", distances[0][i])
        if "null" in tweets.columns:
            print(tweets.iloc[indices[0][i]])
        else:
            print(tweets.iloc[indices[0][i]][:-1])

query = input("Enter the query: ")
search_similar_sentences(query)